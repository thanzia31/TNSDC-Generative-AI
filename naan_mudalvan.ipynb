{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b155767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6253 images belonging to 2 classes.\n",
      "Found 6221 images belonging to 2 classes.\n",
      "195/195 [==============================] - 1176s 6s/step - loss: 0.1695 - accuracy: 0.9363 - val_loss: 0.1750 - val_accuracy: 0.9412\n",
      "195/195 [==============================] - 435s 2s/step - loss: 0.1797 - accuracy: 0.9396\n",
      "Validation Accuracy: 0.9395595788955688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2515b86a710>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore' )\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import json\n",
    "from PIL import ImageFile,Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "def create_model():\n",
    "    dataset_dir = \"D:/imagemod\"\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        os.path.join(dataset_dir, 'val1'),\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "         base_model,\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size\n",
    "    )\n",
    "\n",
    "    evaluation_result = model.evaluate(validation_generator)\n",
    "    \n",
    "    evaluation_dict = {\n",
    "        \"validation_loss\": evaluation_result[0],\n",
    "        \"validation_accuracy\": evaluation_result[1]\n",
    "    }\n",
    "\n",
    "    json_data = json.dumps(evaluation_dict)\n",
    "    print(\"Validation Accuracy:\", evaluation_result[1])\n",
    "    return model\n",
    "\n",
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d16116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Approved\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def input_model_image(img_path):\n",
    "    train_model = False\n",
    "\n",
    "    if train_model:\n",
    "        model = create_model()\n",
    "    else:\n",
    "        model_path = \"D:/image_moderation_model.h5\"\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    return prediction\n",
    "\n",
    "img_path = 'C:/Users/i310thgeN/Documents/webpage/djangoproject/blog/static/images/fashion.jpg'\n",
    "prediction = input_model_image(img_path)\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print('Image not approved')\n",
    "else:\n",
    "    print('Image Approved')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "011f7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ac0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "496/496 [==============================] - 119s 217ms/step - loss: 0.4546 - accuracy: 0.8499 - val_loss: 0.2896 - val_accuracy: 0.9070\n",
      "Epoch 2/5\n",
      "496/496 [==============================] - 104s 210ms/step - loss: 0.2287 - accuracy: 0.9225 - val_loss: 0.3169 - val_accuracy: 0.8954\n",
      "Epoch 3/5\n",
      "496/496 [==============================] - 104s 210ms/step - loss: 0.1253 - accuracy: 0.9573 - val_loss: 0.3736 - val_accuracy: 0.8860\n",
      "Epoch 4/5\n",
      "496/496 [==============================] - 111s 223ms/step - loss: 0.0625 - accuracy: 0.9800 - val_loss: 0.4974 - val_accuracy: 0.8858\n",
      "Epoch 5/5\n",
      "496/496 [==============================] - 108s 217ms/step - loss: 0.0390 - accuracy: 0.9877 - val_loss: 0.5285 - val_accuracy: 0.8835\n",
      "155/155 [==============================] - 7s 42ms/step - loss: 0.5801 - accuracy: 0.8794\n",
      "Test Accuracy: 0.8793625235557556\n",
      "Model saved.\n",
      "Tokenizer and LabelEncoder saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x2100417c970>,\n",
       " <keras.preprocessing.text.Tokenizer at 0x2101385f460>,\n",
       " LabelEncoder())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('D:/labeled_data.csv')\n",
    "#print(data)\n",
    "data = data.dropna()\n",
    "#X = ['hi', 'idiot', 'u are black']\n",
    "#y = [2, 0, 1]\n",
    "X=data['tweet']\n",
    "y=data['class']\n",
    "#print('X',X)\n",
    "#print(y)\n",
    "def create_and_train_model(X, y):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "    X_padded = pad_sequences(sequences)\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_padded.shape[1]))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=len(np.unique(y_encoded)), activation='softmax'))  \n",
    "\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    model.save('D:/content_moderation_model.h5')\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    \n",
    "    with open('D:/tokenizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "\n",
    "    with open('D:/ label_encoder.pkl', 'wb') as file:\n",
    "        pickle.dump(label_encoder, file)\n",
    "\n",
    "    print(\"Tokenizer and LabelEncoder saved.\")\n",
    "\n",
    "    \n",
    "\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "create_and_train_model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d8d72ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Tokenizer, and LabelEncoder loaded.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Inappropriate Content\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "warnings.filterwarnings('ignore' )\n",
    "import pickle\n",
    "def predict_with_model(model, tokenizer, label_encoder, input_text):\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_sequence = pad_sequences(input_sequence, maxlen=model.input_shape[1])\n",
    "\n",
    "   \n",
    "    predicted_probabilities = model.predict(padded_sequence)\n",
    "    predicted_class = np.argmax(predicted_probabilities)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    confidence = predicted_probabilities[0][predicted_class]\n",
    "\n",
    "    if confidence <= 0.6:\n",
    "        predicted_label=2\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "def load_saved_model():\n",
    "    \n",
    "    loaded_model = load_model('D:/content_moderation_model.h5')\n",
    "\n",
    "    \n",
    "    with open('D:/tokenizer.pkl', 'rb') as file:\n",
    "        loaded_tokenizer = pickle.load(file)\n",
    "\n",
    "    with open('D:/ label_encoder.pkl', 'rb') as file:  # Corrected file name\n",
    "        loaded_label_encoder = pickle.load(file)\n",
    "\n",
    "    print(\"Model, Tokenizer, and LabelEncoder loaded.\")\n",
    "    return loaded_model, loaded_tokenizer, loaded_label_encoder\n",
    "\n",
    "\n",
    "data=pd.read_csv('D:/labeled_data.csv')\n",
    "\n",
    "X=data['tweet']\n",
    "y=data['class']\n",
    "\n",
    "def input_model(content):\n",
    "   \n",
    "    prediction=[]\n",
    "    train_model=False\n",
    "    if train_model:\n",
    "        trained_model, tokenizer, label_encoder = create_and_train_model(X, y)\n",
    "    else:\n",
    "        trained_model, tokenizer, label_encoder = load_saved_model()\n",
    "\n",
    "    result_string = content.replace('\"', '').replace(\"'\", '')\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    p_tag = soup.find_all('p')\n",
    "    #print(\"p_tag\",p_tag)\n",
    "    for i in p_tag:\n",
    "        #print(i)\n",
    "        cleaned_text = i.get_text(separator=' ', strip=True) if i else ''\n",
    "        #print(cleaned_text)\n",
    "        a=predict_with_model(trained_model, tokenizer, label_encoder, cleaned_text)\n",
    "        prediction.append(a)\n",
    "        #print(\"Predictions\",prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "a=input_model('<p>You are nothing but a waste of space in this bloody world... idiot i hate u so much you black people dont belong here hell dammit shit bloody losers</p>')\n",
    "for i in a:\n",
    "    if(i==0 or i==1):\n",
    "        print(\"Inappropriate Content\")\n",
    "    else:\n",
    "        print(\"Accepted Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb4c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
